from __future__ import annotations

"""Utility script for mixing audio files.

``mix_audio`` analyzes each track to estimate tempo and key and chooses
transition parameters that can optionally be guided by an ``emotion``.
The module also supports JSON instructions that describe how multiple
stems should be processed and combined.  These instructions can be
generated by the LLM interface and make use of :mod:`audio.dsp_engine`
functions for simple automation.
"""

import argparse
import json
from pathlib import Path
from typing import Any

import numpy as np
import yaml

try:  # pragma: no cover - optional dependency
    import soundfile as sf
except Exception:  # pragma: no cover - optional dependency
    sf = None  # type: ignore

from MUSIC_FOUNDATION.qnl_utils import quantum_embed
from . import audio_ingestion, dsp_engine

EMOTION_MAP = Path(__file__).resolve().parent.parent / "emotion_music_map.yaml"


def _load(path: Path) -> tuple[np.ndarray, int]:
    if sf is None:
        raise RuntimeError("soundfile library not installed")
    data, sr = sf.read(path, always_2d=False)
    return np.asarray(data, dtype=float), sr


def _load_emotion_map() -> dict:
    try:
        with EMOTION_MAP.open("r") as f:
            return yaml.safe_load(f) or {}
    except FileNotFoundError:  # pragma: no cover - configuration optional
        return {}


def mix_audio(
    paths: list[Path], emotion: str | None = None
) -> tuple[np.ndarray, int, dict[str, float | str | None]]:
    """Return mixed audio and transition info.

    Tempo and key are estimated for each track. When ``emotion`` is provided,
    tempo/key from :mod:`emotion_music_map` override the analyzed values.
    The returned ``info`` dictionary contains the chosen ``tempo`` and ``key``.
    """

    data, sr = _load(paths[0])
    mix = np.zeros_like(data, dtype=float)
    tempos: list[float] = []
    keys: list[str | None] = []
    for p in paths:
        d, s = _load(p)
        if s != sr:
            raise ValueError("sample rates differ")
        tempos.append(audio_ingestion.extract_tempo(d, s))
        keys.append(audio_ingestion.extract_key(d))
        if d.shape[0] > mix.shape[0]:
            mix = np.pad(mix, (0, d.shape[0] - mix.shape[0]))
        if d.shape[0] < mix.shape[0]:
            d = np.pad(d, (0, mix.shape[0] - d.shape[0]))
        mix += d
    mix /= len(paths)

    tempo = float(np.mean(tempos)) if tempos else 0.0
    key = keys[0] if keys else None

    if emotion:
        mapping = _load_emotion_map()
        emot_info = mapping.get(emotion, {})
        tempo = float(emot_info.get("tempo", tempo))
        key = emot_info.get("scale", key)

    return mix, sr, {"tempo": tempo, "key": key}


def _apply_dsp(data: np.ndarray, sr: int, params: dict[str, Any]) -> tuple[np.ndarray, int]:
    """Apply basic DSP operations described by ``params``."""

    if "pitch" in params:
        data, sr = dsp_engine.pitch_shift(data, sr, float(params["pitch"]))
    if "time" in params:
        data, sr = dsp_engine.time_stretch(data, sr, float(params["time"]))
    if "compress" in params:
        comp = params["compress"] or {}
        thresh = float(comp.get("threshold", -18.0))
        ratio = float(comp.get("ratio", 2.0))
        data, sr = dsp_engine.compress(data, sr, thresh, ratio)
    return data, sr


def mix_from_instructions(instr: dict[str, Any]) -> tuple[np.ndarray, int]:
    """Return mixed audio according to ``instr`` JSON structure.

    ``instr`` follows the format::

        {
            "stems": [
                {"file": "path.wav", "pitch": 1.0, "time": 1.2,
                 "compress": {"threshold": -18, "ratio": 2}}
            ],
            "output": "mix.wav",
            "preview": {"file": "preview.wav", "duration": 1.0}
        }

    The function returns the mixed audio and sample rate.
    Writing to disk is handled by :func:`main`.
    """

    stems = instr.get("stems", [])
    if not stems:
        raise ValueError("no stems provided")

    first = Path(stems[0]["file"])
    mix, sr = _load(first)
    mix, sr = _apply_dsp(mix, sr, stems[0])
    max_len = mix.shape[0]
    buffers = [mix]

    for stem in stems[1:]:
        data, s = _load(Path(stem["file"]))
        if s != sr:
            raise ValueError("sample rates differ")
        data, s = _apply_dsp(data, s, stem)
        max_len = max(max_len, data.shape[0])
        buffers.append(data)

    mix = np.zeros(max_len, dtype=float)
    for buf in buffers:
        if buf.shape[0] < max_len:
            buf = np.pad(buf, (0, max_len - buf.shape[0]))
        mix += buf
    mix /= len(buffers)
    return mix, sr


def main(args: list[str] | None = None) -> None:
    parser = argparse.ArgumentParser()
    parser.add_argument("files", nargs="*")
    parser.add_argument("--instructions", type=Path)
    parser.add_argument("--output")
    parser.add_argument("--preview")
    parser.add_argument("--preview-duration", type=float, default=1.0)
    parser.add_argument("--qnl-text")
    parser.add_argument("--emotion")
    opts = parser.parse_args(args)

    if opts.instructions is not None:
        with opts.instructions.open() as f:
            instr = json.load(f)
        mix, sr = mix_from_instructions(instr)
        if sf is None:
            raise RuntimeError("soundfile library not installed")
        out_dir = Path("output")
        out_dir.mkdir(parents=True, exist_ok=True)
        out_name = instr.get("output", "final.wav")
        sf.write(out_dir / out_name, mix, sr, subtype="PCM_16")
        prev = instr.get("preview", {})
        prev_name = prev.get("file", "preview.wav")
        dur = int(sr * float(prev.get("duration", 1.0)))
        sf.write(out_dir / prev_name, mix[:dur], sr, subtype="PCM_16")
        return

    if not opts.files or not opts.output:
        parser.error("files and --output required unless --instructions is used")

    if opts.qnl_text:
        quantum_embed(opts.qnl_text)

    mix, sr, info = mix_audio([Path(f) for f in opts.files], opts.emotion)
    if sf is None:
        raise RuntimeError("soundfile library not installed")
    sf.write(opts.output, mix, sr, subtype="PCM_16")
    if opts.preview:
        dur = int(sr * opts.preview_duration)
        sf.write(opts.preview, mix[:dur], sr, subtype="PCM_16")


if __name__ == "__main__":
    main()
